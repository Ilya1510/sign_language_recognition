{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "letters = {}\n",
    "from string import ascii_lowercase\n",
    "for c in ascii_lowercase:\n",
    "    letters[c] = c;\n",
    "del letters['j']\n",
    "del letters['z']\n",
    "letters = sorted(list(letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collectImages(letter):\n",
    "    speakers = ['A', 'B', 'C', 'D', 'E']\n",
    "    data = []\n",
    "    for s in speakers:\n",
    "            image_path = 'dataset5/' + s + '/' + letter + '/'\n",
    "            names = glob.glob(image_path + \"*.*\")\n",
    "            for name in names:\n",
    "                image = cv2.imread(name)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                h = image.shape[0]\n",
    "                w = image.shape[1]\n",
    "                if h > w:\n",
    "                    if (w/h > 0.8):\n",
    "                        k = int((h - w) / 2)\n",
    "                        image = image[k:-k-1, :]\n",
    "                    else:\n",
    "                        image = image[int(0.1*h): -int(0.1*h)-1, :]\n",
    "                if h < w:\n",
    "                    if (h/w > 0.8):\n",
    "                        k = int((w - h) / 2)\n",
    "                        image = image[:, k:-k-1]\n",
    "                    else:\n",
    "                        image = image[:, int(0.1*h): -int(0.1*h)-1]\n",
    "                image = cv2.resize(image, (128, 128))\n",
    "                data.append(image)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = collectImages('a')\n",
    "plt.figure(figsize=(15 ,15))\n",
    "for i in range(len(data)):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    imgplot = plt.imshow(data[i])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "cv2.startWindowThread()\n",
    "cv2.namedWindow(\"image\")\n",
    "cv2.imshow('image',data[1])\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hog_descriptor = cv2.HOGDescriptor((128, 128), #winsize\n",
    "                                    (16, 16), #blocksize\n",
    "                                    (8, 8), #blockstride\n",
    "                                    (8, 8),   #cellsize\n",
    "                                    9)        #nbins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "f = hog_descriptor.compute(data[0]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precountSize(count):\n",
    "    speakers = ['A', 'B', 'C', 'D', 'E']\n",
    "    result = 0\n",
    "    for l in letters[:count]:\n",
    "        for s in speakers:\n",
    "            image_path = 'dataset5/' + s + '/' + l + '/'\n",
    "            names = glob.glob(image_path + \"*.*\")\n",
    "            result += len(names)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LETTERS_TO_ANALYSE = 2 #берём первые 5 букв\n",
    "\n",
    "HOGSIZE = 142884\n",
    "dim1 = 252\n",
    "dim2 = 189\n",
    "dim3 = 3\n",
    "# dim1 * dim2 * dim3 = HOGSIZE\n",
    "HOGSHAPE = (dim1, dim2, dim3)\n",
    "ANS_SIZE = LETTERS_TO_ANALYSE # или len(letters)\n",
    "IMG_COUNT = precountSize(LETTERS_TO_ANALYSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.empty((IMG_COUNT, dim1, dim2, dim3))\n",
    "Y = np.empty((IMG_COUNT, 1))\n",
    "cur = 0\n",
    "for i in range(LETTERS_TO_ANALYSE):\n",
    "    Ldata = collectImages(letters[i])\n",
    "    #Answer = np.zeros(ANS_SIZE)\n",
    "    #Answer[i] = 1\n",
    "    \n",
    "    for Im in Ldata:\n",
    "        X[cur] = np.reshape(hog_descriptor.compute(Im), HOGSHAPE)\n",
    "        Y[cur] = i\n",
    "        cur += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5404, 1)\n",
      "(5404, 2)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "print(Y.shape)\n",
    "Y = to_categorical(Y, num_classes=ANS_SIZE)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_order = np.random.permutation(X.shape[0])\n",
    "Y.astype('float32')\n",
    "x_shuffle = []\n",
    "y_shuffle = []\n",
    "for i in random_order:\n",
    "    x_shuffle.append(X[i])\n",
    "    y_shuffle.append(Y[i])\n",
    "X = np.array(x_shuffle) \n",
    "Y = np.array(y_shuffle)\n",
    "split = int(round(0.2*len(Y)))\n",
    "x_train = X[split:]\n",
    "y_train = Y[split:]\n",
    "x_test = X[:split]\n",
    "y_test = Y[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications import InceptionV3\n",
    "basic = applications.InceptionV3(weights=None, include_top=False, input_shape=HOGSHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 49152)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3072)              150998016 \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 192)               590016    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 24)                4632      \n",
      "=================================================================\n",
      "Total params: 151,592,664\n",
      "Trainable params: 151,592,664\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "newmodel = Sequential()\n",
    "newmodel.add(Flatten(input_shape = basic.output_shape[1:]))\n",
    "newmodel.add(Dense(3072, activation='relu'))\n",
    "newmodel.add(Dense(192, activation='relu'))\n",
    "newmodel.add(Dense(24, activation='relu'))\n",
    "newmodel.summary()\n",
    "final = Model(inputs = basic.input, outputs = newmodel(basic.output))\n",
    "final.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "#final.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "b_size = 16\n",
    "eps = 30\n",
    "\n",
    "from keras_tqdm import TQDMNotebookCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6001ae12f172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdatagen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m final.fit_generator(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdatagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mb_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.5/site-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow\u001b[0;34m(self, x, y, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0msave_to_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_to_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0msave_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m             save_format=save_format)\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m     def flow_from_directory(self, directory,\n",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.5/site-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, image_data_generator, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format)\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0mdata_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \"\"\"\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator()\n",
    "final.fit_generator(\n",
    "    datagen.flow(x_train, y_train, batch_size=b_size),\n",
    "    steps_per_epoch=x_train.shape[0] // b_size,\n",
    "    epochs=eps,\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[ModelCheckpoint('HandsRecognition.model', monitor='val_acc', save_best_only=True), TQDMNotebookCallback()],\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base_model = applications.InceptionV3(weights=None, include_top=False, input_shape=HOGSIZE)\n",
    "add_model = Sequential()\n",
    "add_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "add_model.add(Dense(256, activation='relu'))\n",
    "add_model.add(Dense(24, activation='sigmoid'))\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=add_model(base_model.output))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=30, \n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1, \n",
    "        horizontal_flip=True)\n",
    "train_datagen.fit(x_train)\n",
    "\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[ModelCheckpoint('mod.model', monitor='val_acc', save_best_only=True)]\n",
    ")\n",
    "\n",
    "#model.fit(x_train, y_train, batch_size=b_size, epochs = 30, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model.predict(x_test, y_test, batch_size=b_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
